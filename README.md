## ðŸ”— Links
[![twitter](https://img.shields.io/badge/twitter-1DA1F2?style=for-the-badge&logo=twitter&logoColor=white)](https://twitter.com/Bneayoub1)

dont forget to follow me on twitter (Bneayoub1, link above) to travel with me :) 
# 100 Codings Days
Building BNEAY simple Chatbot.
This project is way for me to talk about what i'm passionate about (machine learning), Learn new things related to that field, and at the same time apply what I'm learning currently in software engineering to build a simple chatbot and deploy it on a website for people to test.

Stay tuned for the whole duration of this projects to travel and discover new stuff with me this whole journey.

## Day 1: (1st April 2023)
I took a peak into Natural Language Processing (NLP), That I still will be learning it's theoritical backround in the upcoming days before moving to applying with its implementation in Python. and I have identified some main steps to achieve it, the first one is Tokenization:
### Tokenization:
Tokenization is the process of breaking down a text into individual words or phrases, known as tokens. This is an essential step in many natural language processing (NLP) applications, as it allows the system to work with discrete units of text rather than attempting to analyze the entire text as a whole.

There are several methods for tokenization, but the most common approach is to split the text at the spaces between words. However, this simple approach can be problematic for languages that don't use spaces between words, such as Chinese and Japanese.

To handle these cases, more advanced tokenization techniques may be used, such as using machine learning algorithms to identify word boundaries based on patterns in the text. This approach can be more accurate but also more complex to implement.

Once the text has been tokenized, it can be used as input to other NLP processes such as part of speech tagging, parsing, and sentiment analysis. Tokenization is therefore a critical step in many NLP applications and is an important concept to understand for anyone working with natural language data. All these Notions that has been cited i this paragraph will be detailed later on, before moving on to the application of all concepts. Stay Tuuuuuned !! :)

## Day 2 & 3: (2d and 3d April 2023)
I started checking for ways to collect Data. One way seemed to be getting a ready file of Data that has at least 1 million characters from Kaggle, one file seemed to be really interesting was a Shakespeare transcript with all shakespeare work. This alternative is easier and time reducing for this step, but I'll keep it as a second plan.
My first plan is to try web scraping the famous forum "Reddit". Reddit has a lot of text and lot of conversations and comments on different topics. Getting this data, clean it, and structure it the right way may be really the way to got to build a consistant chat bot. for the next day, that would be the approach to follow.